{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88dd7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa43964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>case_folding</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sejak ada penambahan traffic light laluan perg...</td>\n",
       "      <td>sejak ada penambahan traffic light laluan perg...</td>\n",
       "      <td>tambah traffic light lalu pergi kerja kena ban...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>355 hari Presiden @prabowo Monitor Yth Bpk Pre...</td>\n",
       "      <td>355 hari presiden @prabowo monitor yth bpk pre...</td>\n",
       "      <td>presiden monitor yth bpk presiden impor singga...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Bambangmulyonoo Malaysia punya kilang dimaksi...</td>\n",
       "      <td>@bambangmulyonoo malaysia punya kilang dimaksi...</td>\n",
       "      <td>malaysia maksimal diindo import negri tetangga...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@LahrianSyahilla Pertamina juga memperkuat sek...</td>\n",
       "      <td>@lahriansyahilla pertamina juga memperkuat sek...</td>\n",
       "      <td>kuat sektor hilir tingkat kapasitas efisiensi ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalau overcapacity kilang seharusnya produksi ...</td>\n",
       "      <td>kalau overcapacity kilang seharusnya produksi ...</td>\n",
       "      <td>overcapacity produksi limpah stock cukup impor...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>@SUARAKRISTEN1 @detikcom Kilang Dumai Pertamin...</td>\n",
       "      <td>@suarakristen1 @detikcom kilang dumai pertamin...</td>\n",
       "      <td>dumai milik kapasitas olah mentah barel dasar ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>@tanyakanrl Aku pake azarine ijo tosca yg 45sp...</td>\n",
       "      <td>@tanyakanrl aku pake azarine ijo tosca yg 45sp...</td>\n",
       "      <td>pake azarine ijo tosca spf pake bikin kulit ku...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>@tanyakanrl Aku pakai amaterasun yg biru tua n...</td>\n",
       "      <td>@tanyakanrl aku pakai amaterasun yg biru tua n...</td>\n",
       "      <td>pakai amaterasun biru tua nder tahun kayak pak...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Pertamina Ungkap Proyek RDMP Balikpapan Siap L...</td>\n",
       "      <td>pertamina ungkap proyek rdmp balikpapan siap l...</td>\n",
       "      <td>proyek rdmp balikpapan laku uji operasi rfcc b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Pertamina lagi gaspol nih Proyek RDMP Balikpap...</td>\n",
       "      <td>pertamina lagi gaspol nih proyek rdmp balikpap...</td>\n",
       "      <td>gaspol nih proyek rdmp balikpapan masuk tahap ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             full_text  \\\n",
       "0    Sejak ada penambahan traffic light laluan perg...   \n",
       "1    355 hari Presiden @prabowo Monitor Yth Bpk Pre...   \n",
       "2    @Bambangmulyonoo Malaysia punya kilang dimaksi...   \n",
       "3    @LahrianSyahilla Pertamina juga memperkuat sek...   \n",
       "4    Kalau overcapacity kilang seharusnya produksi ...   \n",
       "..                                                 ...   \n",
       "245  @SUARAKRISTEN1 @detikcom Kilang Dumai Pertamin...   \n",
       "246  @tanyakanrl Aku pake azarine ijo tosca yg 45sp...   \n",
       "247  @tanyakanrl Aku pakai amaterasun yg biru tua n...   \n",
       "248  Pertamina Ungkap Proyek RDMP Balikpapan Siap L...   \n",
       "249  Pertamina lagi gaspol nih Proyek RDMP Balikpap...   \n",
       "\n",
       "                                          case_folding  \\\n",
       "0    sejak ada penambahan traffic light laluan perg...   \n",
       "1    355 hari presiden @prabowo monitor yth bpk pre...   \n",
       "2    @bambangmulyonoo malaysia punya kilang dimaksi...   \n",
       "3    @lahriansyahilla pertamina juga memperkuat sek...   \n",
       "4    kalau overcapacity kilang seharusnya produksi ...   \n",
       "..                                                 ...   \n",
       "245  @suarakristen1 @detikcom kilang dumai pertamin...   \n",
       "246  @tanyakanrl aku pake azarine ijo tosca yg 45sp...   \n",
       "247  @tanyakanrl aku pakai amaterasun yg biru tua n...   \n",
       "248  pertamina ungkap proyek rdmp balikpapan siap l...   \n",
       "249  pertamina lagi gaspol nih proyek rdmp balikpap...   \n",
       "\n",
       "                                         preprocessing     label  \n",
       "0    tambah traffic light lalu pergi kerja kena ban...  negative  \n",
       "1    presiden monitor yth bpk presiden impor singga...  negative  \n",
       "2    malaysia maksimal diindo import negri tetangga...  negative  \n",
       "3    kuat sektor hilir tingkat kapasitas efisiensi ...  positive  \n",
       "4    overcapacity produksi limpah stock cukup impor...  negative  \n",
       "..                                                 ...       ...  \n",
       "245  dumai milik kapasitas olah mentah barel dasar ...  positive  \n",
       "246  pake azarine ijo tosca spf pake bikin kulit ku...  positive  \n",
       "247  pakai amaterasun biru tua nder tahun kayak pak...  positive  \n",
       "248  proyek rdmp balikpapan laku uji operasi rfcc b...  positive  \n",
       "249  gaspol nih proyek rdmp balikpapan masuk tahap ...  positive  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('labeled_manual.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33d83ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stop_words = set(stopwords.words(\"indonesian\"))\n",
    "\n",
    "custom_stopwords = {\n",
    "    \"nya\", \"sih\", \"saja\", \"aja\", \"kok\", \"sama\", \"pun\", \"lah\", \"dong\", \"deh\",\n",
    "    \"banget\", \"bgt\", \"yg\", \"ya\", \"kan\", \"kah\", \"nah\", \"loh\", \"tuh\", \"nih\",\n",
    "    \"gitu\", \"gituan\", \"begitu\", \"oke\", \"ok\", \"yaa\", \"yaaah\", \"yah\", \"iyaa\",\n",
    "    \"iya\", \"engga\", \"nggak\", \"ngga\", \"gak\", \n",
    "}\n",
    "\n",
    "stop_words.update(custom_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d620308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"slang.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        abbreviation_dict = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: File 'dopping.json' tidak ditemukan using empty dict.\")\n",
    "    abbreviation_dict = {}\n",
    "\n",
    "# 2. Compile Regex Patterns (Dilakukan di awal agar efisien)\n",
    "PATTERN_WORD_ONLY = re.compile(r'^[a-z]+$')\n",
    "PATTERN_URL = re.compile(r'http\\S+|www\\S+')\n",
    "PATTERN_USER_HASHTAG = re.compile(r'[@#]\\w+')\n",
    "PATTERN_NON_ALPHA = re.compile(r'[^a-z\\s]')\n",
    "PATTERN_REPEATED_CHAR = re.compile(r'(.)\\1{2,}')\n",
    "\n",
    "def reduce_repeated_chars(word):\n",
    "    \"\"\"Mengubah 'haaaalo' menjadi 'halo'\"\"\"\n",
    "    return PATTERN_REPEATED_CHAR.sub(r'\\1', word)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Fungsi utama preprocessing\"\"\"\n",
    "    \n",
    "    # Cek validitas input\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # --- Tahap Cleaning Awal ---\n",
    "    text = text.lower()\n",
    "    text = PATTERN_URL.sub(' ', text)          # Hapus URL\n",
    "    text = emoji.replace_emoji(text, replace=' ') # Hapus Emoji\n",
    "    text = PATTERN_USER_HASHTAG.sub(' ', text) # Hapus @username dan #hashtag\n",
    "    \n",
    "    # Hapus karakter non-latin/ASCII (Clean up aneh-aneh)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Hapus selain huruf a-z dan spasi\n",
    "    text = PATTERN_NON_ALPHA.sub(' ', text)\n",
    "\n",
    "    # --- Tahap Tokenizing & Processing ---\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "\n",
    "    for w in words:\n",
    "        # 1. Hapus repetisi karakter\n",
    "        w = reduce_repeated_chars(w)\n",
    "\n",
    "        # 2. Normalisasi singkatan (jika ada di dict)\n",
    "        w = abbreviation_dict.get(w, w)\n",
    "\n",
    "        # 3. Validasi: Pastikan hanya huruf (redundant check, tapi aman)\n",
    "        if not PATTERN_WORD_ONLY.match(w):\n",
    "            continue\n",
    "\n",
    "        # 4. Stopword Removal Tahap 1 (Sebelum stemming untuk efisiensi)\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "\n",
    "        # 5. Stemming\n",
    "        w = stemmer.stem(w)\n",
    "\n",
    "        # 6. Stopword Removal Tahap 2 (Cek lagi setelah di-stem)\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "            \n",
    "        cleaned_words.append(w)\n",
    "\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "# Contoh penggunaan\n",
    "# df['cleaned_text'] = df['text_column'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d703902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      traffic light pergi kerja kena bangun extra sb...\n",
       "1      presiden monitor yth bpk presiden impor singga...\n",
       "2      malaysia maksimal diindo import negri tetangga...\n",
       "3      kuat sektor hilir tingkat kapasitas efisiensi ...\n",
       "4      overcapacity produksi limpah stock impor harga...\n",
       "                             ...                        \n",
       "245    dumai milik kapasitas olah mentah barel dasar ...\n",
       "246    pake azarine ijo tosca spf pake kulit kusam ce...\n",
       "247    pakai amaterasun biru tua nder kayak pakai rin...\n",
       "248    proyek rdmp balikpapan laku uji operasi rfcc p...\n",
       "249    gaspol proyek rdmp balikpapan masuk tahap uji ...\n",
       "Name: bersih, Length: 250, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"bersih\"] = df[\"preprocessing\"].apply(clean_text)\n",
    "df['bersih']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbaed68",
   "metadata": {},
   "source": [
    "### <h1>Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c025c9",
   "metadata": {},
   "source": [
    "<h1>1. BoW + Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0ddd934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERCOBAAN 1: Bag of Words (BoW) + Naive Bayes ===\n",
      "\n",
      "======================================\n",
      "Akurasi Bow + Naive Bayes 0.72\n",
      "======================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.72      0.78        25\n",
      "     neutral       0.67      0.29      0.40         7\n",
      "    positive       0.62      0.89      0.73        18\n",
      "\n",
      "    accuracy                           0.72        50\n",
      "   macro avg       0.71      0.63      0.64        50\n",
      "weighted avg       0.74      0.72      0.71        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"\\n=== PERCOBAAN 1: Bag of Words (BoW) + Naive Bayes ===\")\n",
    "\n",
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['bersih'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Extraction: BoW\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# Model: Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_bow, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_bow)\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"Akurasi Bow + Naive Bayes\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"======================================\")\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7048932",
   "metadata": {},
   "source": [
    "<h1>2. Word2Vec + Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f5cd2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERCOBAAN 2: Word2Vec + Naive Bayes ===\n",
      "\n",
      "======================================\n",
      "Akurasi Word2Vec + Naive Bayes 0.5\n",
      "======================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.36      0.49        25\n",
      "     neutral       0.25      0.14      0.18         7\n",
      "    positive       0.44      0.83      0.58        18\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.48      0.45      0.42        50\n",
      "weighted avg       0.57      0.50      0.48        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"\\n=== PERCOBAAN 2: Word2Vec + Naive Bayes ===\")\n",
    "\n",
    "# --- 1. Training Word2Vec ---\n",
    "# Tokenisasi kalimat menjadi list kata\n",
    "sentences = [text.split() for text in df['bersih']]\n",
    "\n",
    "# Latih model Word2Vec\n",
    "# vector_size=100 (dimensi), window=5, min_count=1\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Fungsi untuk mengubah kalimat menjadi rata-rata vektor kata\n",
    "def sent_vectorizer(sent, model):\n",
    "    words = sent.split()\n",
    "    # Ambil vektor untuk setiap kata yang ada di vocabulary model\n",
    "    vector_list = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if len(vector_list) > 0:\n",
    "        # Rata-rata vektor kata\n",
    "        return np.mean(vector_list, axis=0)\n",
    "    else:\n",
    "        # Jika tidak ada kata yang dikenal, kembalikan vektor nol\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Terapkan vectorizer ke data\n",
    "X = np.array([sent_vectorizer(text, w2v_model) for text in df['bersih']])\n",
    "y = df['label']\n",
    "\n",
    "# --- 2. Klasifikasi Naive Bayes ---\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model GaussianNB (Cocok untuk data kontinu seperti Word2Vec)\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"Akurasi Word2Vec + Naive Bayes\", accuracy_score(y_test, y_pred))\n",
    "print(\"======================================\")\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2adbf",
   "metadata": {},
   "source": [
    "### <h1>SVM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb6bdf",
   "metadata": {},
   "source": [
    "<h1>1. TF-IDF + SVM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df38a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERCOBAAN 2: TF-IDF + SVM ===\n",
      "\n",
      "======================================\n",
      "Akurasi TF-IDF + SVM 0.8\n",
      "======================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        25\n",
      "     neutral       0.75      0.43      0.55         7\n",
      "    positive       0.71      0.83      0.77        18\n",
      "\n",
      "    accuracy                           0.80        50\n",
      "   macro avg       0.78      0.71      0.73        50\n",
      "weighted avg       0.80      0.80      0.79        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"\\n=== PERCOBAAN 2: TF-IDF + SVM ===\")\n",
    "\n",
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['bersih'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Extraction: TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Model: SVM\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"Akurasi TF-IDF + SVM\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"======================================\")\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c72dc",
   "metadata": {},
   "source": [
    "<h1>2. Word2Vec + SVM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa4693ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERCOBAAN 4: Word2Vec + SVM ===\n",
      "\n",
      "======================================\n",
      "Akurasi Word2Vec + SVM 0.36\n",
      "======================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        25\n",
      "     neutral       0.00      0.00      0.00         7\n",
      "    positive       0.36      1.00      0.53        18\n",
      "\n",
      "    accuracy                           0.36        50\n",
      "   macro avg       0.12      0.33      0.18        50\n",
      "weighted avg       0.13      0.36      0.19        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"\\n=== PERCOBAAN 4: Word2Vec + SVM ===\")\n",
    "\n",
    "# --- 1. Training Word2Vec ---\n",
    "sentences = [text.split() for text in df['bersih']]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    words = sent.split()\n",
    "    vector_list = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vector_list) > 0:\n",
    "        return np.mean(vector_list, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X = np.array([sent_vectorizer(text, w2v_model) for text in df['bersih']])\n",
    "y = df['label']\n",
    "\n",
    "# --- 2. Klasifikasi SVM ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model SVM\n",
    "svm = SVC(kernel='linear') # Bisa diganti 'rbf'\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"\\n======================================\")\n",
    "print(\"Akurasi Word2Vec + SVM\", accuracy_score(y_test, y_pred))\n",
    "print(\"======================================\")\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1ceb2",
   "metadata": {},
   "source": [
    "### <h1>Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a440ea",
   "metadata": {},
   "source": [
    "<h1>1. TF-IDF + Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb02fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERCOBAAN 5: TF-IDF + Logistic Regression ===\n",
      "\n",
      "======================================\n",
      "Akurasi TF-IDF + Logistic Regression 0.82\n",
      "======================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.88      0.90        25\n",
      "     neutral       0.75      0.43      0.55         7\n",
      "    positive       0.73      0.89      0.80        18\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.80      0.73      0.75        50\n",
      "weighted avg       0.83      0.82      0.81        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n=== PERCOBAAN 5: TF-IDF + Logistic Regression ===\")\n",
    "\n",
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['bersih'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model: Logistic Regression (Menggunakan fitur TF-IDF dari percobaan sebelumnya)\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"Akurasi TF-IDF + Logistic Regression\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"======================================\")\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a861d04",
   "metadata": {},
   "source": [
    "<h1>2. Word2Vec + Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd684429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERCOBAAN 5: Word2Vec + Logistic Regression ===\n",
      "\n",
      "======================================\n",
      "Akurasi Word2Vec + Logistic Regression 0.36\n",
      "======================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        25\n",
      "     neutral       0.00      0.00      0.00         7\n",
      "    positive       0.36      1.00      0.53        18\n",
      "\n",
      "    accuracy                           0.36        50\n",
      "   macro avg       0.12      0.33      0.18        50\n",
      "weighted avg       0.13      0.36      0.19        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"\\n=== PERCOBAAN 5: Word2Vec + Logistic Regression ===\")\n",
    "\n",
    "# --- 1. Training Word2Vec ---\n",
    "sentences = [text.split() for text in df['bersih']]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    words = sent.split()\n",
    "    vector_list = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vector_list) > 0:\n",
    "        return np.mean(vector_list, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X = np.array([sent_vectorizer(text, w2v_model) for text in df['bersih']])\n",
    "y = df['label']\n",
    "\n",
    "# --- 2. Klasifikasi Logistic Regression ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"Akurasi Word2Vec + Logistic Regression\", accuracy_score(y_test, y_pred))\n",
    "print(\"======================================\")\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119ef4a",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81250fb0",
   "metadata": {},
   "source": [
    "<h1>1. Word2Vec + simple RNN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f429beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERCOBAAN 6: Word2Vec + RNN ===\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.3933 - loss: 1.0903 - val_accuracy: 0.3800 - val_loss: 1.0446\n",
      "Epoch 2/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4435 - loss: 1.0962 - val_accuracy: 0.3600 - val_loss: 1.0478\n",
      "Epoch 3/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4099 - loss: 1.0867 - val_accuracy: 0.5600 - val_loss: 1.0277\n",
      "Epoch 4/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4172 - loss: 1.0830 - val_accuracy: 0.3800 - val_loss: 1.0367\n",
      "Epoch 5/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3756 - loss: 1.0744 - val_accuracy: 0.3600 - val_loss: 1.0710\n",
      "Epoch 6/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3176 - loss: 1.1033 - val_accuracy: 0.4000 - val_loss: 1.0555\n",
      "Epoch 7/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4465 - loss: 1.0544 - val_accuracy: 0.3800 - val_loss: 1.0381\n",
      "Epoch 8/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4378 - loss: 1.0658 - val_accuracy: 0.4000 - val_loss: 1.0313\n",
      "Epoch 9/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3999 - loss: 1.1047 - val_accuracy: 0.5000 - val_loss: 1.0396\n",
      "Epoch 10/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4159 - loss: 1.0562 - val_accuracy: 0.4400 - val_loss: 1.0412\n",
      "Epoch 11/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4170 - loss: 1.0678 - val_accuracy: 0.4200 - val_loss: 1.0488\n",
      "Epoch 12/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4645 - loss: 1.0420 - val_accuracy: 0.5400 - val_loss: 1.0492\n",
      "Epoch 13/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4366 - loss: 1.0613 - val_accuracy: 0.5000 - val_loss: 1.0496\n",
      "Epoch 14/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5051 - loss: 1.0127 - val_accuracy: 0.3600 - val_loss: 1.0481\n",
      "Epoch 15/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5037 - loss: 1.0120 - val_accuracy: 0.5000 - val_loss: 1.0330\n",
      "Akurasi RNN: 0.5\n",
      "\n",
      "======================================\n",
      "Akurasi Word2Vec + RNN 0.5\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\n=== PERCOBAAN 6: Word2Vec + RNN ===\")\n",
    "\n",
    "# --- 1. Persiapan Data & Word2Vec ---\n",
    "# Label Encoding (Ubah label teks ke angka)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['label'])\n",
    "num_classes = len(np.unique(y_encoded))\n",
    "\n",
    "# Training Word2Vec\n",
    "sentences = [text.split() for text in df['bersih']]\n",
    "w2v_size = 100\n",
    "w2v_model = Word2Vec(sentences, vector_size=w2v_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Tokenisasi untuk Deep Learning\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['bersih'])\n",
    "X_seq = tokenizer.texts_to_sequences(df['bersih'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding (menyamakan panjang kalimat)\n",
    "max_length = 20 \n",
    "X_pad = pad_sequences(X_seq, maxlen=max_length)\n",
    "\n",
    "# Membuat Embedding Matrix dari bobot Word2Vec\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    else:\n",
    "        # Jika kata tidak ada di W2V, biarkan nol atau random (opsional)\n",
    "        pass\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. Membangun Model RNN ---\n",
    "model = Sequential()\n",
    "# Layer Embedding menggunakan bobot dari Word2Vec (trainable=False agar bobot tidak rusak saat awal training)\n",
    "model.add(Embedding(vocab_size, w2v_size, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='softmax')) # Softmax untuk multi-kelas\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluasi\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Akurasi RNN: {accuracy}\")\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"Akurasi Word2Vec + RNN\", accuracy)\n",
    "print(\"======================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
